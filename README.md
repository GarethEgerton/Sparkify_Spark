# Sparkify_Spark

## Summary

Data for a music streaming startup resides in S3. The S3 directory contains JSON logs of user activity as well as JSON metadata of the songs on the app.

* An ETL pipeline was built to extract the data from S3, process using Spark and load back into S3 as a set of dimensional tables to allow the analytics to run queries and analyse the data.
* The dimensional tables are organised in a star schema to facilitate simple and quick querying by business users. Joins paths are clear via the fact table.
* The ETL process was deployed on a cluster using AWS EMR.

### Song data:
* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data

This is a subset of the Million Song Dataset: http://millionsongdataset.com/ \
Stored in folders of the form:
* song_data/A/B/C/TRABCEI128F424C983.json
* song_data/A/A/B/TRAABJL12903CDCF1A.json

### Log data:
Generated by an event simulator: https://github.com/Interana/eventsim \
Stored in folders of the form:
* log_data/2018/11/2018-11-12-events.json
* log_data/2018/11/2018-11-13-events.json

### Star Schema
Tables created as follows:

***Fact Table***
1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
* songplay_id
* start_time
* user_id
* level
* song_id
* artist_id
* session_id
* location
* user_agent

***Dimension Tables***

2. **users** - users in the app
* user_id 
* first_name 
* last_name
* gender 
* level
3. **songs** - songs in music database
* song_id 
* title 
* artist_id 
* year 
* duration
4. **artists** - artists in music database
* artist_id
* name 
* location  
* lattitude
* longitude
5. **time** - timestamps of records in songplays broken down into specific units
* start_time 
* hour 
* day 
* week 
* month 
* year 
* weekday
  
### Files
* etl.py
* dl.cfg

### Instructions

**Make sure that AWS CLI is installed**

If not already installed: 

        pip install awscli

Configure AWS CLI with your AWS credentials (store in folder .aws within your home folder location), run the following command and enter your credentuals

        aws configure

**Create Cluster and run etl.py**

        aws emr create-cluster --name sparkify_cluster --use-default-roles --release-label emr-5.28.0 --instance-count 6 --applications Name=Spark --ec2-attributes KeyName=gaga_oregon --instance-type m5.xlarge --steps Type=spark,Name=Sparkify,Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.yarn.submit.waitAppCompletion=True,s3://gigilake/etl.py],ActionOnFailure=CONTINUE --auto-terminate
        
This performs the following steps:
* Creates cluster named sparkify_cluster
* Loads 6 instances, 1 master node and 5 core nodes
* Specifies aws keyname
* Choose instance type
* Specifies steps to take after cluster loads
  * Spark app called Sparkify, python script located at s3://gigilake/etl.py
  * Instructs to wait for app to finish execution and then auto terminate cluster

### Files

### etl.py

* Starts a Spark Session
* Reads in data from S3
* Transforms data into specified star schema tables
* Writes 5 tables to output folder in S3 as parquet files











